# Optimization_Implementation

## Project Title
# Description
This project implements various optimization algorithms for machine learning models, including gradient descent with mini-batch, full batch, and stochastic approaches. It also incorporates the use of momentum, AdaGrad optimizer, Adam optimizer, and BFGS algorithm.

## Features
1- Gradient descent optimization with mini-batch, full batch, and stochastic approaches
2- Momentum optimization
3- AdaGrad optimizer
4- Adam optimizer
5- BFGS algorithm
