# Optimization_Implementation

## Project Title
# Description
This project implements various optimization algorithms for machine learning models, including gradient descent with mini-batch, full batch, and stochastic approaches. It also incorporates the use of momentum, AdaGrad optimizer, Adam optimizer, and BFGS algorithm.

## Features
- Gradient descent optimization with mini-batch, full batch, and stochastic approaches
- Momentum optimization
- AdaGrad optimizer
- Adam optimizer
- BFGS algorithm
